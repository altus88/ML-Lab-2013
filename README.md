2013-DL-GennadyShabanov
=======================
Code contains the following implementations (also see readme in the corresponding folders):

1. Multinomial logistic regression. Evaluation of the implementation on MNIST data. Visualization of the 10 receptive fields. Plot of the error curves for the training, evaluation and test set.
2. A neural network with one hidden layer. Minibatch gradient descent training and rmsprop. Evaluation of the implementation on MNIST. Visualization of the receptive fields of the first hidden layer, plots of the error curves. Numerical gradient check. 
3. PCA and sparse Autoencoder (http://ufldl.stanford.edu/wiki/index.php/Autoencoders_and_Sparsity). PCA scatterplot (http://peekaboo-vision.blogspot.de/2012/12/another-look-at-mnist.html) on MNIST, and on CIFAR-10 (http://www.cs.toronto.edu/~kriz/cifar.html). Visualization of the learnt filters.
4. Running t-SNE using the new Barnes-Hut implementation (http://homepage.tudelft.nl/19j49/t-SNE_files/bh_tsne.tar.gz) which allows to produce 2D embeddingâ€™s for a large number of samples.
5. K-Means. Implementation of steps on page 5 in the paper http://www.stanford.edu/~acoates/papers/coatesng_nntot2012.pdf. Visualization of the filters on CIFAR-10 (rescaled images from 32x32 to 12x12 pixels). Implementation of minibatch K-Means. 


